import json
import time
import pendulum

from google.cloud import bigquery
from google.cloud import storage
from google.cloud.exceptions import NotFound

# GCP configuration
GCP_PROJECT = 'affable-alpha-450915-s8'
GCS_BUCKET = BQ_DATASET = 'devoteam_assignment'
GCS_FOLDER = 'dummy_data'
BQ_TABLE = 'killer_table'
STAGING_TZ = 'Europe/Amsterdam'

# Supported datatypes
bq_schema_mapping = {
    str: 'STRING',
    float: 'FLOAT',
    int: 'INTEGER',
    bool: 'BOOLEAN',
    type(pendulum.now()): 'TIMESTAMP'
}

def determine_schema(record, schema):
    for name, value in record.items():
        if name in schema and not isinstance(value, dict): continue
        mode = 'NULLABLE'
        if isinstance(value, list):
            value = value[0]
            mode = 'REPEATED'
        if isinstance(value, dict):
            schema[name] = {
                'type': 'RECORD',
                'mode': mode,
                'fields': determine_schema(value, schema.get(name, {}).get('fields', {})),
            }
        else:
            if isinstance(value, str):
                try: value = pendulum.parse(value)
                except: pass
            schema[name] = {
                'type': bq_schema_mapping[type(value)],
                'mode': mode
            }
    return schema

def fix_timestamps(record, schema):
    for name, value in record.items():
        data_type = schema[name]['type']
        if data_type == 'RECORD':
            record[name] = fix_timestamps(value, schema[name]['fields'])
        elif data_type == 'TIMESTAMP':
            value = pendulum.parse(value, tz=STAGING_TZ)
            record[name] = value.to_datetime_string()
    return record

def schema_dict_to_bq(schema):
    return [bigquery.SchemaField(
        name=field,
        field_type=value['type'],
        mode=value['mode'],
        fields=schema_dict_to_bq(value.get('fields'))
    ) for field, value in schema.items()] if schema else None

def schema_bq_to_dict(schema):
    return {field.name: {
        'type': field.field_type,
        'mode': field.mode,
        'fields': schema_bq_to_dict(field.fields)
    } for field in schema} if schema else None

def merge_schemas(schema_data, schema_bq):
    merged = schema_data | schema_bq
    for field, config in schema_data.items():
        if config.get('fields') and schema_bq.get(field):
            sub_merged = merge_schemas(config['fields'], schema_bq[field]['fields'])
            merged[field]['fields'] = sub_merged
    return merged


# Initialise the GCP clients
gcs_client = storage.Client(GCP_PROJECT)
bq_client = bigquery.Client(GCP_PROJECT)

# Read the GCS data that simulates the output of an ingestion run
blobs = gcs_client.list_blobs(GCS_BUCKET)
files, data = [blob for blob in blobs if blob.name.startswith(GCS_FOLDER) and blob.name.endswith('.json')], []
for file in files:
    with file.open('r') as in_file:
        # Keeping the files separated in the list to avoid creating 10Mb+ imports that BQ would reject
        data += [[json.loads(record) for record in in_file]]

# Derive the schema from the available data
schema = {}
for record in [record for file in data for record in file]:
    schema = determine_schema(record, schema)

# Reformat the timestamps so that they are ready for BigQuery
new_data = []
for file in data:
    new_file = []
    for record in file:
        new_record = fix_timestamps(record, schema)
        new_file.append(new_record)
    new_data += [new_file]

table_id = f'{GCP_PROJECT}.{BQ_DATASET}.{BQ_TABLE}'
try:
    # If table exists, push any newly discovered fields
    table = bq_client.get_table(table_id)
    table.schema = schema_dict_to_bq(merge_schemas(schema, schema_bq_to_dict(table.schema)))
    table = bq_client.update_table(table, ['schema'])
except NotFound:
    # We assume that ts is always generated by the ingestion process
    schema['ts']['mode'] = 'REQUIRED'
    # If table does not exist, create it
    table = bq_client.create_table(bigquery.Table(table_id, schema_dict_to_bq(schema)))
    time.sleep(10)
    bq_client = bigquery.Client(GCP_PROJECT)

i, sleep_duration = 0, 5
while i <= 10:
    try:
        # Insert the new data into the BigQuery table
        for file_contents in new_data:
            errors = bq_client.insert_rows_json(table_id, file_contents)
            if errors: raise Exception('Error(s) occurred while inserting rows:\n{}'.format(errors))
    except NotFound:
        print('Table not yet available, waiting {} seconds...'.format(sleep_duration))
        time.sleep(sleep_duration)
        i += 1
        sleep_duration *= 2
