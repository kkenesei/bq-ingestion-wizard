from datetime import datetime
import json

from google.cloud import bigquery
from google.cloud import storage
from google.cloud.exceptions import NotFound

timestamp_formats = [
    '%Y-%m-%dT%H:%M:%S',    # for ts
    '%Y-%m-%d',             # for dates in API data
    '%Y-%m-%dT%H:%M:%S%z'   # for timestamps in API data
]

bq_schema_mapping = {
    str: 'STRING',
    float: 'FLOAT',
    int: 'INTEGER',
    bool: 'BOOLEAN',
    datetime: 'TIMESTAMP'
}

def determine_schema(field, schema):
    for field, value in field.items():
        if field in schema and not isinstance(value, dict):
            continue
        mode = 'NULLABLE'
        if isinstance(value, list):
            value = value[0]
            mode = 'REPEATED'
        if isinstance(value, dict):
            schema[field] = {
                'type': 'RECORD',
                'mode': mode,
                'fields': determine_schema(value, schema.get(field, {}).get('fields', {})),
            }
        else:
            if isinstance(value, str):
                for format in timestamp_formats:
                    try:
                        value = datetime.strptime(value, format)
                    except:
                        continue
            schema[field] = {
                'type': bq_schema_mapping[type(value)],
                'mode': mode
            }
    return schema

def schema_dict_to_bq(schema):
    return [bigquery.SchemaField(
        name=field,
        field_type=value['type'],
        mode=value['mode'],
        fields=schema_dict_to_bq(value.get('fields'))
    ) for field, value in schema.items()] if schema else None

def schema_bq_to_dict(schema):
    return {field.name: {
        'type': field.field_type,
        'mode': field.mode,
        'fields': schema_bq_to_dict(field.fields)
    } for field in schema} if schema else None

def merge_schemas(schema_data, schema_bq):
    merged = schema_bq | schema_data
    for field in schema_data.keys():
        if schema_data[field].get('fields') and schema_bq.get(field):
            sub_merged = merge_schemas(schema_data[field]['fields'], schema_data[field]['fields'])
            merged[field]['fields'] = sub_merged
    return merged


# Initialise the GCP clients
storage_client = storage.Client('devoteam-assignment')
client = bigquery.Client('devoteam-assignment')

# Read the GCS data that simulates the output of an ingestion run
blobs = storage_client.list_blobs('devoteam_assignment')
files, data = [blob for blob in blobs if blob.name.startswith('dummy_data') and blob.name.endswith('.json')], []
for file in files:
    with file.open('r') as in_file:
        data += [json.loads(record) for record in in_file]

# For debugging purposes: possibility of local import
# with open('dummy_data/dummy_data_1.json') as in_file:
#     data = [json.loads(record) for record in in_file]

# Assemble the complete list of fields along with JSON-compatible data types
schema = {}
for record in data:
    schema = determine_schema(record, schema)

# We assume that ts is always generated by the ingestion process
schema['ts']['mode'] = 'REQUIRED'

table_id = 'affable-alpha-450915-s8.mega_dataset.killer_table'
try:
    # If table exists, push any newly discovered fields
    table = client.get_table(table_id)
    table.schema = schema_dict_to_bq(merge_schemas(schema, schema_bq_to_dict(table.schema)))
    table = client.update_table(table, ['schema'])
except NotFound:
    # If table does not exist, create it
    table = client.create_table(bigquery.Table(table_id, schema_dict_to_bq(schema)))

# Todo: load data into BQ table
